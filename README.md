# NLP-with-Disaster-Tweets

This repository contains my solution for the [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/code/ghaithzaza/nlp-with-disaster-tweets-roberta) competition on Kaggle.

## Overview
The goal of this competition is to predict which tweets are about real disasters and which ones are not.

## Model Choice: RoBERTa
I chose RoBERTa (Robustly Optimized BERT Approach) for this task because:
- It's an optimized version of BERT with improved training methodology
- Better performance on natural language understanding tasks
- Handles informal text (like tweets) well due to its dynamic masking
- Strong contextual understanding of language nuances
- Pre-trained on a larger dataset than BERT

## Performance
- Achieved score of 0.84094 on the test set
- Model showed strong ability to distinguish between real disaster tweets and false alarms
- Performed well even with informal language and social media shorthand

## Links
- [Kaggle Notebook](https://www.kaggle.com/code/ghaithzaza/nlp-with-disaster-tweets-roberta)
